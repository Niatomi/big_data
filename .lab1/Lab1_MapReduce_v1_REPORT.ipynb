{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "    <center>МИНИСТЕРСТВО НАУКИ И ВЫСШЕГО ОБРАЗОВАНИЯ</center>\n",
    "    <center>федеральное государственное автономное образовательное учреждение высшего образования </center> <center>«Самарский национальный исследовательский университет имени академика С.П. Королева»</center>\n",
    "    <center>(Самарский университет)</center> </br>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<br>\n",
    "<center>Институт \t     информатики и кибернетики</center>                                                   \t  \n",
    "<center>Кафедра \t     технической кибернетики</center>                                                              \t\n",
    "</br>\n",
    "<br/>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br>\n",
    "<center>ОТЧЕТ</center>\n",
    "<center>по лабораторной работе №1</center>\n",
    "\n",
    "<center>«Введение в MapReduce модель на Python»</center>\n",
    "<br/>\n",
    "<center>по дисциплине <strong>«Большие данные»</strong></center>\n",
    "<br/>\n",
    "<center></center>\n",
    "</br>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<br>\n",
    "<p style=\"text-align:right;\">Обучающийся: Скляров Д.В.\n",
    "<br>6131-010402D\n",
    "<br>    \n",
    "<br>Преподаватель: Попов С.Б.\n",
    "</p>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "    <br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<center>Самара 2024</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82OvPKEiEqjc"
   },
   "source": [
    "# Введение в MapReduce модель на Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JQ2cvXLjICmI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nia/Desktop/ssau/big_data/lab1\n"
     ]
    }
   ],
   "source": [
    "from typing import (\n",
    "    NamedTuple,\n",
    "    Iterator,\n",
    "    Any,\n",
    ")\n",
    "from random import randint\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VAL = 100\n",
    "ARR_SIZE = 100\n",
    "input_vals = [randint(0, MAX_VAL - 1) for _ in range(ARR_SIZE)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfvAeZm3S8S8"
   },
   "source": [
    "### Максимальное значение ряда\n",
    "\n",
    "Разработайте MapReduce алгоритм, который находит максимальное число входного списка чисел.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "iv = input_vals.copy()\n",
    "\n",
    "\n",
    "class CTX:\n",
    "    CTX_STORAGE = {}\n",
    "\n",
    "    def __setattr__(self, name: str, value: Any) -> None:\n",
    "        self.CTX_STORAGE[name] = value\n",
    "\n",
    "    def __getattr__(self, attr: str) -> Any:\n",
    "        try:\n",
    "            return self.CTX_STORAGE[attr]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def ctx_inject(func):\n",
    "    ctx = CTX()\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return func(ctx, *args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@ctx_inject\n",
    "def MAP(ctx: CTX, a):\n",
    "    if ctx.max_value == None or ctx.max_value <= a:\n",
    "        ctx.max_value = a\n",
    "        return a\n",
    "\n",
    "\n",
    "@ctx_inject\n",
    "def REDUCE(ctx: CTX, val):\n",
    "    if val is not None and ctx.max_value == val:\n",
    "        yield val\n",
    "\n",
    "\n",
    "def flatten(nested_iterable):\n",
    "    for iterable in nested_iterable:\n",
    "        for element in iterable:\n",
    "            yield element\n",
    "\n",
    "\n",
    "max_value,  = list(flatten(list(map(REDUCE, (map(MAP, iv))))))\n",
    "assert max_value == max(iv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k86bXnqZTk-U"
   },
   "source": [
    "### Арифметическое среднее\n",
    "\n",
    "Разработайте MapReduce алгоритм, который находит арифметическое среднее.\n",
    "\n",
    "$$\\overline{X} = \\frac{1}{n}\\sum_{i=0}^{n} x_i$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "\n",
    "@ctx_inject\n",
    "def combine(ctx, value):\n",
    "    s, c = value\n",
    "    try:\n",
    "        ctx.sum += s\n",
    "        ctx.cnt += c\n",
    "    except TypeError:\n",
    "        ctx.sum = s\n",
    "        ctx.cnt = c\n",
    "    return (ctx.sum, ctx.cnt)\n",
    "\n",
    "\n",
    "def REDUCE(value):\n",
    "    s, c = value\n",
    "    return s / c\n",
    "\n",
    "\n",
    "map_reduce_avg = list(\n",
    "    map(REDUCE, (map(combine, (map(lambda x: (x, 1), input_vals))))))[-1]\n",
    "assert map_reduce_avg == statistics.mean(input_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xanzszhsIlLe"
   },
   "source": [
    "### GroupByKey на основе сортировки\n",
    "\n",
    "Реализуйте groupByKey на основе сортировки, проверьте его работу на примерах\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hQPn3USsIkEC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'female': [User(id=1, age=25, social_contacts=240, gender='female'),\n",
      "            User(id=2, age=25, social_contacts=500, gender='female'),\n",
      "            User(id=3, age=33, social_contacts=800, gender='female')],\n",
      " 'male': [User(id=0, age=55, social_contacts=20, gender='male')]}\n"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple  # requires python 3.6+\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "class User(NamedTuple):\n",
    "    id: int\n",
    "    age: str\n",
    "    social_contacts: int\n",
    "    gender: str\n",
    "\n",
    "\n",
    "input_collection = [\n",
    "    User(id=0, age=55, gender='male', social_contacts=20),\n",
    "    User(id=1, age=25, gender='female', social_contacts=240),\n",
    "    User(id=2, age=25, gender='female', social_contacts=500),\n",
    "    User(id=3, age=33, gender='female', social_contacts=800)\n",
    "]\n",
    "\n",
    "\n",
    "@ctx_inject\n",
    "def clear_ctx(ctx):\n",
    "    ctx.agreagregation = {}\n",
    "\n",
    "\n",
    "@ctx_inject\n",
    "def REDUCE(ctx, value):\n",
    "    key, agg = value\n",
    "    if ctx.agreagregation is None:\n",
    "        ctx.agreagregation = {}\n",
    "\n",
    "    aggregs = ctx.agreagregation.get(key, [])\n",
    "    aggregs.append(agg)\n",
    "    ctx.agreagregation[key] = aggregs\n",
    "    return ctx.agreagregation\n",
    "\n",
    "\n",
    "KEY = 'gender'\n",
    "clear_ctx()\n",
    "pprint(list(map(REDUCE, list(\n",
    "    map(lambda x: (x.__getattribute__(KEY), x), input_collection))))[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SgEjCZyGnu6"
   },
   "source": [
    "### Drop duplicates (set construction, unique elements, distinct)\n",
    "\n",
    "Реализуйте распределённую операцию исключения дубликатов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "okjbyApjGhMt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[User(id=0, age=55, social_contacts=20, gender='male'),\n",
       " User(id=1, age=25, social_contacts=240, gender='female')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@ctx_inject\n",
    "def map_phase_1(ctx, value: User):\n",
    "    if ctx.existing_pairs is None:\n",
    "        ctx.existing_pairs = []\n",
    "\n",
    "    for field in value._fields:\n",
    "        par = (field, value.__getattribute__(field))\n",
    "        if par not in ctx.existing_pairs:\n",
    "            yield par[0]\n",
    "            ctx.existing_pairs.append(par)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "@ctx_inject\n",
    "def clear_ctx(ctx):\n",
    "    ctx.existing_pairs = None\n",
    "\n",
    "\n",
    "UNIQUE_BY = 'gender'\n",
    "\n",
    "\n",
    "def reduce(factors, collection):\n",
    "    global UNIQUE_BY\n",
    "    if UNIQUE_BY in list(factors):\n",
    "        yield collection\n",
    "\n",
    "\n",
    "clear_ctx()\n",
    "list(flatten(list(map(reduce, list(\n",
    "    map(map_phase_1, input_collection)), input_collection))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7sRGoTXuJze"
   },
   "source": [
    "## Операторы реляционной алгебры\n",
    "\n",
    "### Selection (Выборка)\n",
    "\n",
    "**The Map Function**: Для каждого кортежа $t \\in R$ вычисляется истинность предиката $C$. В случае истины создаётся пара ключ-значение $(t, t)$. В паре ключ и значение одинаковы, равны $t$.\n",
    "\n",
    "**The Reduce Function:** Роль функции Reduce выполняет функция идентичности, которая возвращает то же значение, что получила на вход.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4nKIKe59uIfc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{80},\n",
       " {8},\n",
       " {78},\n",
       " {97},\n",
       " {10},\n",
       " {16},\n",
       " {26},\n",
       " {31},\n",
       " {3},\n",
       " {75},\n",
       " {3},\n",
       " {33},\n",
       " {31},\n",
       " {93},\n",
       " {94},\n",
       " {25},\n",
       " {3},\n",
       " {80},\n",
       " {25},\n",
       " {59},\n",
       " {29},\n",
       " {62},\n",
       " {9},\n",
       " {0},\n",
       " {20},\n",
       " {76},\n",
       " {0},\n",
       " {4},\n",
       " {6},\n",
       " {80},\n",
       " {4},\n",
       " {76},\n",
       " {93},\n",
       " {58},\n",
       " {67},\n",
       " {39},\n",
       " {33},\n",
       " {75},\n",
       " {25},\n",
       " {13},\n",
       " {11},\n",
       " {75},\n",
       " {25},\n",
       " {90},\n",
       " {96},\n",
       " {20},\n",
       " {11}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def C(x): return True if x >= 0 else False\n",
    "\n",
    "\n",
    "input_vals = [randint(-100, 100) for _ in range(ARR_SIZE)]\n",
    "\n",
    "\n",
    "def MAP(pred, t):\n",
    "    if pred:\n",
    "        yield {t, t}\n",
    "\n",
    "\n",
    "list(flatten(map(lambda x: x, map(MAP, map(C, input_vals), input_vals))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gau6lKXvn2R"
   },
   "source": [
    "### Union (Объединение)\n",
    "\n",
    "**The Map Function:** Превратите каждый входной кортеж $t$ в пару ключ-значение $(t, t)$.\n",
    "\n",
    "**The Reduce Function:** С каждым ключом $t$ будет ассоциировано одно или два значений. В обоих случаях создайте $(t, t)$ в качестве выходного значения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Sns7a5agv3nw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-87: [-87, -75],\n",
       " -46: [-46, 49],\n",
       " 57: [57, -52],\n",
       " 91: [91, -3],\n",
       " -74: [-74, -55],\n",
       " -44: [-44, -12],\n",
       " -90: [-90, 81],\n",
       " -11: [-11, 38],\n",
       " 33: [33, -12],\n",
       " 52: [52, -35],\n",
       " 79: [79, 49],\n",
       " -99: [-99, -10],\n",
       " 12: [12, -52],\n",
       " -98: [-98, -69],\n",
       " -50: [-50, 33],\n",
       " 30: [30, 60],\n",
       " -64: [-64, 98],\n",
       " 96: [96, -96],\n",
       " -7: [-7, -78],\n",
       " -100: [-100, -100],\n",
       " -57: [-57, -11],\n",
       " 47: [47, -54],\n",
       " 66: [66],\n",
       " 22: [22, 71],\n",
       " 38: [38, 16],\n",
       " -45: [-45, -59],\n",
       " -20: [-20, 66],\n",
       " 73: [73, -58],\n",
       " -31: [-31, 29],\n",
       " -6: [-6, -92],\n",
       " -79: [-79, -17],\n",
       " -40: [-40, 39],\n",
       " 8: [8, 22],\n",
       " -21: [-21, 64],\n",
       " 11: [11, 8],\n",
       " -30: [-30, 23],\n",
       " 26: [26, 79],\n",
       " -4: [-4, 32],\n",
       " 35: [35, 9],\n",
       " 90: [90, -58],\n",
       " -70: [-70, 46],\n",
       " 64: [64, 64],\n",
       " -58: [-58, -72]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "input_vals = [(randint(-100, 100), ) for _ in range(99)]\n",
    "\n",
    "\n",
    "@ctx_inject\n",
    "def reduce(ctx, x: dict[int, int]):\n",
    "    if ctx.union is None:\n",
    "        ctx.union = {}\n",
    "        ctx.last_key = None\n",
    "    ctx.union: dict\n",
    "    if len(ctx.union.keys()) >= 1:\n",
    "        if len(ctx.union[ctx.last_key]) == 1:\n",
    "            for val in x.values():\n",
    "                ctx.union[ctx.last_key].append(val)\n",
    "        else:\n",
    "            for val in x.values():\n",
    "                ctx.union[val] = [val]\n",
    "                ctx.last_key = val\n",
    "    else:\n",
    "        for val in x.values():\n",
    "            ctx.union[val] = [val]\n",
    "            ctx.last_key = val\n",
    "    return ctx.union\n",
    "\n",
    "\n",
    "@ctx_inject\n",
    "def clear_ctx(ctx):\n",
    "    ctx.union = None\n",
    "    ctx.last_key = None\n",
    "\n",
    "\n",
    "clear_ctx()\n",
    "\n",
    "list(map(reduce, map(lambda x: {x[0]: x[0]}, input_vals)))[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03IffTEOJgOb"
   },
   "source": [
    "## Вычисление TF-IDF (Term Frequency – Inverse Document Fraquency)\n",
    "\n",
    "Реализуется в три этапа:\n",
    "\n",
    "**Этап 1:** Частота слова в документе\n",
    "\n",
    "**Этап 2:** Количество документов, в которых встречается слово\n",
    "\n",
    "**Этап 3:** Расчёт TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC1 = \"\"\"\n",
    "Lorem ipsum dolor sit amet consectetur adipisicing elit. Dignissimos labore placeat ut saepe dolorum quisquam sint dolore distinctio vel ratione tempora perspiciatis iusto voluptates quos laudantium at aut recusandae nulla, officia molestias. Quidem commodi rerum itaque placeat recusandae illo. Quo nemo et repellendus in maxime beatae consequatur ullam doloremque debitis earum, quidem aliquid quasi dolorum enim similique id ipsam minus eum corrupti alias cum? Recusandae incidunt nostrum consequatur at sit repudiandae asperiores ab esse ut quos dolore optio debitis doloremque dolorem eum accusamus dolores eos minus, corporis ducimus quasi! Dolorem, sequi et optio aperiam vel ipsam suscipit! Delectus, eos sit!\n",
    "\"\"\"\n",
    "DOC2 = \"\"\"\n",
    "There are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration in some form, by injected humour, or randomised words which don't look even slightly believable. If you are going to use a passage of Lorem Ipsum, you need to be sure there isn't anything embarrassing hidden in the middle of text. All the Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary, making this the first true generator on the Internet. It uses a dictionary of over 200 Latin words, combined with a handful of model sentence structures, to generate Lorem Ipsum which looks reasonable. The generated Lorem Ipsum is therefore always free from repetition, injected humour, or non-characteristic words etc.\n",
    "\"\"\"\n",
    "DOC3 = \"\"\"\n",
    "It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal distribution distribution of letters\n",
    "\"\"\"\n",
    "\n",
    "docs = ['DOC1', 'DOC2', 'DOC3']\n",
    "contents = [DOC1, DOC2, DOC3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Lorem', 'DOC1'), 0.004771212547196624),\n",
       " (('ipsum', 'DOC1'), 0.004771212547196624),\n",
       " (('dolor', 'DOC1'), 0.004771212547196624),\n",
       " (('sit', 'DOC1'), 0.009542425094393249),\n",
       " (('amet', 'DOC1'), 0.004771212547196624),\n",
       " (('consectetur', 'DOC1'), 0.004771212547196624),\n",
       " (('adipisicing', 'DOC1'), 0.004771212547196624),\n",
       " (('elit.', 'DOC1'), 0.004771212547196624),\n",
       " (('Dignissimos', 'DOC1'), 0.004771212547196624),\n",
       " (('labore', 'DOC1'), 0.004771212547196624),\n",
       " (('placeat', 'DOC1'), 0.009542425094393249),\n",
       " (('ut', 'DOC1'), 0.009542425094393249),\n",
       " (('saepe', 'DOC1'), 0.004771212547196624),\n",
       " (('dolorum', 'DOC1'), 0.009542425094393249),\n",
       " (('quisquam', 'DOC1'), 0.004771212547196624),\n",
       " (('sint', 'DOC1'), 0.004771212547196624),\n",
       " (('dolore', 'DOC1'), 0.009542425094393249),\n",
       " (('distinctio', 'DOC1'), 0.004771212547196624),\n",
       " (('vel', 'DOC1'), 0.009542425094393249),\n",
       " (('ratione', 'DOC1'), 0.004771212547196624),\n",
       " (('tempora', 'DOC1'), 0.004771212547196624),\n",
       " (('perspiciatis', 'DOC1'), 0.004771212547196624),\n",
       " (('iusto', 'DOC1'), 0.004771212547196624),\n",
       " (('voluptates', 'DOC1'), 0.004771212547196624),\n",
       " (('quos', 'DOC1'), 0.009542425094393249),\n",
       " (('laudantium', 'DOC1'), 0.004771212547196624),\n",
       " (('at', 'DOC1'), 0.009542425094393249),\n",
       " (('aut', 'DOC1'), 0.004771212547196624),\n",
       " (('recusandae', 'DOC1'), 0.009542425094393249),\n",
       " (('nulla,', 'DOC1'), 0.004771212547196624),\n",
       " (('officia', 'DOC1'), 0.004771212547196624),\n",
       " (('molestias.', 'DOC1'), 0.004771212547196624),\n",
       " (('Quidem', 'DOC1'), 0.004771212547196624),\n",
       " (('commodi', 'DOC1'), 0.004771212547196624),\n",
       " (('rerum', 'DOC1'), 0.004771212547196624),\n",
       " (('itaque', 'DOC1'), 0.004771212547196624),\n",
       " (('illo.', 'DOC1'), 0.004771212547196624),\n",
       " (('Quo', 'DOC1'), 0.004771212547196624),\n",
       " (('nemo', 'DOC1'), 0.004771212547196624),\n",
       " (('et', 'DOC1'), 0.009542425094393249),\n",
       " (('repellendus', 'DOC1'), 0.004771212547196624),\n",
       " (('in', 'DOC1'), 0.004771212547196624),\n",
       " (('maxime', 'DOC1'), 0.004771212547196624),\n",
       " (('beatae', 'DOC1'), 0.004771212547196624),\n",
       " (('consequatur', 'DOC1'), 0.009542425094393249),\n",
       " (('ullam', 'DOC1'), 0.004771212547196624),\n",
       " (('doloremque', 'DOC1'), 0.009542425094393249),\n",
       " (('debitis', 'DOC1'), 0.009542425094393249),\n",
       " (('earum,', 'DOC1'), 0.004771212547196624),\n",
       " (('quidem', 'DOC1'), 0.004771212547196624),\n",
       " (('aliquid', 'DOC1'), 0.004771212547196624),\n",
       " (('quasi', 'DOC1'), 0.004771212547196624),\n",
       " (('enim', 'DOC1'), 0.004771212547196624),\n",
       " (('similique', 'DOC1'), 0.004771212547196624),\n",
       " (('id', 'DOC1'), 0.004771212547196624),\n",
       " (('ipsam', 'DOC1'), 0.009542425094393249),\n",
       " (('minus', 'DOC1'), 0.004771212547196624),\n",
       " (('eum', 'DOC1'), 0.009542425094393249),\n",
       " (('corrupti', 'DOC1'), 0.004771212547196624),\n",
       " (('alias', 'DOC1'), 0.004771212547196624),\n",
       " (('cum?', 'DOC1'), 0.004771212547196624),\n",
       " (('Recusandae', 'DOC1'), 0.004771212547196624),\n",
       " (('incidunt', 'DOC1'), 0.004771212547196624),\n",
       " (('nostrum', 'DOC1'), 0.004771212547196624),\n",
       " (('repudiandae', 'DOC1'), 0.004771212547196624),\n",
       " (('asperiores', 'DOC1'), 0.004771212547196624),\n",
       " (('ab', 'DOC1'), 0.004771212547196624),\n",
       " (('esse', 'DOC1'), 0.004771212547196624),\n",
       " (('optio', 'DOC1'), 0.009542425094393249),\n",
       " (('dolorem', 'DOC1'), 0.004771212547196624),\n",
       " (('accusamus', 'DOC1'), 0.004771212547196624),\n",
       " (('dolores', 'DOC1'), 0.004771212547196624),\n",
       " (('eos', 'DOC1'), 0.009542425094393249),\n",
       " (('minus,', 'DOC1'), 0.004771212547196624),\n",
       " (('corporis', 'DOC1'), 0.004771212547196624),\n",
       " (('ducimus', 'DOC1'), 0.004771212547196624),\n",
       " (('quasi!', 'DOC1'), 0.004771212547196624),\n",
       " (('Dolorem,', 'DOC1'), 0.004771212547196624),\n",
       " (('sequi', 'DOC1'), 0.004771212547196624),\n",
       " (('aperiam', 'DOC1'), 0.004771212547196624),\n",
       " (('suscipit!', 'DOC1'), 0.004771212547196624),\n",
       " (('Delectus,', 'DOC1'), 0.004771212547196624),\n",
       " (('sit!', 'DOC1'), 0.004771212547196624),\n",
       " (('Lorem', 'DOC2'), 0.008731797969703203),\n",
       " (('ipsum', 'DOC2'), 0.0014552996616172004),\n",
       " (('dolor', 'DOC2'), 0.0014552996616172004),\n",
       " (('sit', 'DOC2'), 0.002910599323234401),\n",
       " (('amet', 'DOC2'), 0.0014552996616172004),\n",
       " (('consectetur', 'DOC2'), 0.0014552996616172004),\n",
       " (('adipisicing', 'DOC2'), 0.0014552996616172004),\n",
       " (('elit.', 'DOC2'), 0.0014552996616172004),\n",
       " (('Dignissimos', 'DOC2'), 0.0014552996616172004),\n",
       " (('labore', 'DOC2'), 0.0014552996616172004),\n",
       " (('placeat', 'DOC2'), 0.002910599323234401),\n",
       " (('ut', 'DOC2'), 0.002910599323234401),\n",
       " (('saepe', 'DOC2'), 0.0014552996616172004),\n",
       " (('dolorum', 'DOC2'), 0.002910599323234401),\n",
       " (('quisquam', 'DOC2'), 0.0014552996616172004),\n",
       " (('sint', 'DOC2'), 0.0014552996616172004),\n",
       " (('dolore', 'DOC2'), 0.002910599323234401),\n",
       " (('distinctio', 'DOC2'), 0.0014552996616172004),\n",
       " (('vel', 'DOC2'), 0.002910599323234401),\n",
       " (('ratione', 'DOC2'), 0.0014552996616172004),\n",
       " (('tempora', 'DOC2'), 0.0014552996616172004),\n",
       " (('perspiciatis', 'DOC2'), 0.0014552996616172004),\n",
       " (('iusto', 'DOC2'), 0.0014552996616172004),\n",
       " (('voluptates', 'DOC2'), 0.0014552996616172004),\n",
       " (('quos', 'DOC2'), 0.002910599323234401),\n",
       " (('laudantium', 'DOC2'), 0.0014552996616172004),\n",
       " (('at', 'DOC2'), 0.002910599323234401),\n",
       " (('aut', 'DOC2'), 0.0014552996616172004),\n",
       " (('recusandae', 'DOC2'), 0.002910599323234401),\n",
       " (('nulla,', 'DOC2'), 0.0014552996616172004),\n",
       " (('officia', 'DOC2'), 0.0014552996616172004),\n",
       " (('molestias.', 'DOC2'), 0.0014552996616172004),\n",
       " (('Quidem', 'DOC2'), 0.0014552996616172004),\n",
       " (('commodi', 'DOC2'), 0.0014552996616172004),\n",
       " (('rerum', 'DOC2'), 0.0014552996616172004),\n",
       " (('itaque', 'DOC2'), 0.0014552996616172004),\n",
       " (('illo.', 'DOC2'), 0.0014552996616172004),\n",
       " (('Quo', 'DOC2'), 0.0014552996616172004),\n",
       " (('nemo', 'DOC2'), 0.0014552996616172004),\n",
       " (('et', 'DOC2'), 0.002910599323234401),\n",
       " (('repellendus', 'DOC2'), 0.0014552996616172004),\n",
       " (('in', 'DOC2'), 0.004365898984851601),\n",
       " (('maxime', 'DOC2'), 0.0014552996616172004),\n",
       " (('beatae', 'DOC2'), 0.0014552996616172004),\n",
       " (('consequatur', 'DOC2'), 0.002910599323234401),\n",
       " (('ullam', 'DOC2'), 0.0014552996616172004),\n",
       " (('doloremque', 'DOC2'), 0.002910599323234401),\n",
       " (('debitis', 'DOC2'), 0.002910599323234401),\n",
       " (('earum,', 'DOC2'), 0.0014552996616172004),\n",
       " (('quidem', 'DOC2'), 0.0014552996616172004),\n",
       " (('aliquid', 'DOC2'), 0.0014552996616172004),\n",
       " (('quasi', 'DOC2'), 0.0014552996616172004),\n",
       " (('enim', 'DOC2'), 0.0014552996616172004),\n",
       " (('similique', 'DOC2'), 0.0014552996616172004),\n",
       " (('id', 'DOC2'), 0.0014552996616172004),\n",
       " (('ipsam', 'DOC2'), 0.002910599323234401),\n",
       " (('minus', 'DOC2'), 0.0014552996616172004),\n",
       " (('eum', 'DOC2'), 0.002910599323234401),\n",
       " (('corrupti', 'DOC2'), 0.0014552996616172004),\n",
       " (('alias', 'DOC2'), 0.0014552996616172004),\n",
       " (('cum?', 'DOC2'), 0.0014552996616172004),\n",
       " (('Recusandae', 'DOC2'), 0.0014552996616172004),\n",
       " (('incidunt', 'DOC2'), 0.0014552996616172004),\n",
       " (('nostrum', 'DOC2'), 0.0014552996616172004),\n",
       " (('repudiandae', 'DOC2'), 0.0014552996616172004),\n",
       " (('asperiores', 'DOC2'), 0.0014552996616172004),\n",
       " (('ab', 'DOC2'), 0.0014552996616172004),\n",
       " (('esse', 'DOC2'), 0.0014552996616172004),\n",
       " (('optio', 'DOC2'), 0.002910599323234401),\n",
       " (('dolorem', 'DOC2'), 0.0014552996616172004),\n",
       " (('accusamus', 'DOC2'), 0.0014552996616172004),\n",
       " (('dolores', 'DOC2'), 0.0014552996616172004),\n",
       " (('eos', 'DOC2'), 0.002910599323234401),\n",
       " (('minus,', 'DOC2'), 0.0014552996616172004),\n",
       " (('corporis', 'DOC2'), 0.0014552996616172004),\n",
       " (('ducimus', 'DOC2'), 0.0014552996616172004),\n",
       " (('quasi!', 'DOC2'), 0.0014552996616172004),\n",
       " (('Dolorem,', 'DOC2'), 0.0014552996616172004),\n",
       " (('sequi', 'DOC2'), 0.0014552996616172004),\n",
       " (('aperiam', 'DOC2'), 0.0014552996616172004),\n",
       " (('suscipit!', 'DOC2'), 0.0014552996616172004),\n",
       " (('Delectus,', 'DOC2'), 0.0014552996616172004),\n",
       " (('sit!', 'DOC2'), 0.002910599323234401),\n",
       " (('are', 'DOC2'), 0.007886301730903511),\n",
       " (('many', 'DOC2'), 0.003943150865451756),\n",
       " (('variations', 'DOC2'), 0.003943150865451756),\n",
       " (('of', 'DOC2'), 0.023658905192710536),\n",
       " (('passages', 'DOC2'), 0.003943150865451756),\n",
       " (('Ipsum', 'DOC2'), 0.015772603461807023),\n",
       " (('available,', 'DOC2'), 0.003943150865451756),\n",
       " (('but', 'DOC2'), 0.003943150865451756),\n",
       " (('the', 'DOC2'), 0.023658905192710536),\n",
       " (('majority', 'DOC2'), 0.003943150865451756),\n",
       " (('have', 'DOC2'), 0.003943150865451756),\n",
       " (('suffered', 'DOC2'), 0.003943150865451756),\n",
       " (('alteration', 'DOC2'), 0.003943150865451756),\n",
       " (('some', 'DOC2'), 0.003943150865451756),\n",
       " (('form,', 'DOC2'), 0.003943150865451756),\n",
       " (('by', 'DOC2'), 0.003943150865451756),\n",
       " (('injected', 'DOC2'), 0.007886301730903511),\n",
       " (('humour,', 'DOC2'), 0.007886301730903511),\n",
       " (('or', 'DOC2'), 0.007886301730903511),\n",
       " (('randomised', 'DOC2'), 0.003943150865451756),\n",
       " (('words', 'DOC2'), 0.007886301730903511),\n",
       " (('which', 'DOC2'), 0.007886301730903511),\n",
       " ((\"don't\", 'DOC2'), 0.003943150865451756),\n",
       " (('look', 'DOC2'), 0.003943150865451756),\n",
       " (('even', 'DOC2'), 0.003943150865451756),\n",
       " (('slightly', 'DOC2'), 0.003943150865451756),\n",
       " (('believable.', 'DOC2'), 0.003943150865451756),\n",
       " (('If', 'DOC2'), 0.003943150865451756),\n",
       " (('you', 'DOC2'), 0.007886301730903511),\n",
       " (('going', 'DOC2'), 0.003943150865451756),\n",
       " (('to', 'DOC2'), 0.015772603461807023),\n",
       " (('use', 'DOC2'), 0.003943150865451756),\n",
       " (('a', 'DOC2'), 0.011829452596355268),\n",
       " (('passage', 'DOC2'), 0.003943150865451756),\n",
       " (('Ipsum,', 'DOC2'), 0.003943150865451756),\n",
       " (('need', 'DOC2'), 0.003943150865451756),\n",
       " (('be', 'DOC2'), 0.003943150865451756),\n",
       " (('sure', 'DOC2'), 0.003943150865451756),\n",
       " (('there', 'DOC2'), 0.003943150865451756),\n",
       " ((\"isn't\", 'DOC2'), 0.003943150865451756),\n",
       " (('anything', 'DOC2'), 0.003943150865451756),\n",
       " (('embarrassing', 'DOC2'), 0.003943150865451756),\n",
       " (('hidden', 'DOC2'), 0.003943150865451756),\n",
       " (('middle', 'DOC2'), 0.003943150865451756),\n",
       " (('text.', 'DOC2'), 0.003943150865451756),\n",
       " (('All', 'DOC2'), 0.003943150865451756),\n",
       " (('generators', 'DOC2'), 0.003943150865451756),\n",
       " (('on', 'DOC2'), 0.007886301730903511),\n",
       " (('Internet', 'DOC2'), 0.003943150865451756),\n",
       " (('tend', 'DOC2'), 0.003943150865451756),\n",
       " (('repeat', 'DOC2'), 0.003943150865451756),\n",
       " (('predefined', 'DOC2'), 0.003943150865451756),\n",
       " (('chunks', 'DOC2'), 0.003943150865451756),\n",
       " (('as', 'DOC2'), 0.003943150865451756),\n",
       " (('necessary,', 'DOC2'), 0.003943150865451756),\n",
       " (('making', 'DOC2'), 0.003943150865451756),\n",
       " (('this', 'DOC2'), 0.003943150865451756),\n",
       " (('first', 'DOC2'), 0.003943150865451756),\n",
       " (('true', 'DOC2'), 0.003943150865451756),\n",
       " (('generator', 'DOC2'), 0.003943150865451756),\n",
       " (('Internet.', 'DOC2'), 0.003943150865451756),\n",
       " (('It', 'DOC2'), 0.003943150865451756),\n",
       " (('uses', 'DOC2'), 0.003943150865451756),\n",
       " (('dictionary', 'DOC2'), 0.003943150865451756),\n",
       " (('over', 'DOC2'), 0.003943150865451756),\n",
       " (('200', 'DOC2'), 0.003943150865451756),\n",
       " (('Latin', 'DOC2'), 0.003943150865451756),\n",
       " (('words,', 'DOC2'), 0.003943150865451756),\n",
       " (('combined', 'DOC2'), 0.003943150865451756),\n",
       " (('with', 'DOC2'), 0.003943150865451756),\n",
       " (('handful', 'DOC2'), 0.003943150865451756),\n",
       " (('model', 'DOC2'), 0.003943150865451756),\n",
       " (('sentence', 'DOC2'), 0.003943150865451756),\n",
       " (('structures,', 'DOC2'), 0.003943150865451756),\n",
       " (('generate', 'DOC2'), 0.003943150865451756),\n",
       " (('looks', 'DOC2'), 0.003943150865451756),\n",
       " (('reasonable.', 'DOC2'), 0.003943150865451756),\n",
       " (('The', 'DOC2'), 0.003943150865451756),\n",
       " (('generated', 'DOC2'), 0.003943150865451756),\n",
       " (('is', 'DOC2'), 0.003943150865451756),\n",
       " (('therefore', 'DOC2'), 0.003943150865451756),\n",
       " (('always', 'DOC2'), 0.003943150865451756),\n",
       " (('free', 'DOC2'), 0.003943150865451756),\n",
       " (('from', 'DOC2'), 0.003943150865451756),\n",
       " (('repetition,', 'DOC2'), 0.003943150865451756),\n",
       " (('non-characteristic', 'DOC2'), 0.003943150865451756),\n",
       " (('etc.', 'DOC2'), 0.003943150865451756),\n",
       " (('Lorem', 'DOC3'), 0.0),\n",
       " (('ipsum', 'DOC3'), 0.0),\n",
       " (('dolor', 'DOC3'), 0.0),\n",
       " (('sit', 'DOC3'), 0.0),\n",
       " (('amet', 'DOC3'), 0.0),\n",
       " (('consectetur', 'DOC3'), 0.0),\n",
       " (('adipisicing', 'DOC3'), 0.0),\n",
       " (('elit.', 'DOC3'), 0.0),\n",
       " (('Dignissimos', 'DOC3'), 0.0),\n",
       " (('labore', 'DOC3'), 0.0),\n",
       " (('placeat', 'DOC3'), 0.0),\n",
       " (('ut', 'DOC3'), 0.0),\n",
       " (('saepe', 'DOC3'), 0.0),\n",
       " (('dolorum', 'DOC3'), 0.0),\n",
       " (('quisquam', 'DOC3'), 0.0),\n",
       " (('sint', 'DOC3'), 0.0),\n",
       " (('dolore', 'DOC3'), 0.0),\n",
       " (('distinctio', 'DOC3'), 0.0),\n",
       " (('vel', 'DOC3'), 0.0),\n",
       " (('ratione', 'DOC3'), 0.0),\n",
       " (('tempora', 'DOC3'), 0.0),\n",
       " (('perspiciatis', 'DOC3'), 0.0),\n",
       " (('iusto', 'DOC3'), 0.0),\n",
       " (('voluptates', 'DOC3'), 0.0),\n",
       " (('quos', 'DOC3'), 0.0),\n",
       " (('laudantium', 'DOC3'), 0.0),\n",
       " (('at', 'DOC3'), 0.0),\n",
       " (('aut', 'DOC3'), 0.0),\n",
       " (('recusandae', 'DOC3'), 0.0),\n",
       " (('nulla,', 'DOC3'), 0.0),\n",
       " (('officia', 'DOC3'), 0.0),\n",
       " (('molestias.', 'DOC3'), 0.0),\n",
       " (('Quidem', 'DOC3'), 0.0),\n",
       " (('commodi', 'DOC3'), 0.0),\n",
       " (('rerum', 'DOC3'), 0.0),\n",
       " (('itaque', 'DOC3'), 0.0),\n",
       " (('illo.', 'DOC3'), 0.0),\n",
       " (('Quo', 'DOC3'), 0.0),\n",
       " (('nemo', 'DOC3'), 0.0),\n",
       " (('et', 'DOC3'), 0.0),\n",
       " (('repellendus', 'DOC3'), 0.0),\n",
       " (('in', 'DOC3'), 0.0),\n",
       " (('maxime', 'DOC3'), 0.0),\n",
       " (('beatae', 'DOC3'), 0.0),\n",
       " (('consequatur', 'DOC3'), 0.0),\n",
       " (('ullam', 'DOC3'), 0.0),\n",
       " (('doloremque', 'DOC3'), 0.0),\n",
       " (('debitis', 'DOC3'), 0.0),\n",
       " (('earum,', 'DOC3'), 0.0),\n",
       " (('quidem', 'DOC3'), 0.0),\n",
       " (('aliquid', 'DOC3'), 0.0),\n",
       " (('quasi', 'DOC3'), 0.0),\n",
       " (('enim', 'DOC3'), 0.0),\n",
       " (('similique', 'DOC3'), 0.0),\n",
       " (('id', 'DOC3'), 0.0),\n",
       " (('ipsam', 'DOC3'), 0.0),\n",
       " (('minus', 'DOC3'), 0.0),\n",
       " (('eum', 'DOC3'), 0.0),\n",
       " (('corrupti', 'DOC3'), 0.0),\n",
       " (('alias', 'DOC3'), 0.0),\n",
       " (('cum?', 'DOC3'), 0.0),\n",
       " (('Recusandae', 'DOC3'), 0.0),\n",
       " (('incidunt', 'DOC3'), 0.0),\n",
       " (('nostrum', 'DOC3'), 0.0),\n",
       " (('repudiandae', 'DOC3'), 0.0),\n",
       " (('asperiores', 'DOC3'), 0.0),\n",
       " (('ab', 'DOC3'), 0.0),\n",
       " (('esse', 'DOC3'), 0.0),\n",
       " (('optio', 'DOC3'), 0.0),\n",
       " (('dolorem', 'DOC3'), 0.0),\n",
       " (('accusamus', 'DOC3'), 0.0),\n",
       " (('dolores', 'DOC3'), 0.0),\n",
       " (('eos', 'DOC3'), 0.0),\n",
       " (('minus,', 'DOC3'), 0.0),\n",
       " (('corporis', 'DOC3'), 0.0),\n",
       " (('ducimus', 'DOC3'), 0.0),\n",
       " (('quasi!', 'DOC3'), 0.0),\n",
       " (('Dolorem,', 'DOC3'), 0.0),\n",
       " (('sequi', 'DOC3'), 0.0),\n",
       " (('aperiam', 'DOC3'), 0.0),\n",
       " (('suscipit!', 'DOC3'), 0.0),\n",
       " (('Delectus,', 'DOC3'), 0.0),\n",
       " (('sit!', 'DOC3'), 0.0),\n",
       " (('are', 'DOC3'), 0.00858981751491128),\n",
       " (('many', 'DOC3'), 0.00429490875745564),\n",
       " (('variations', 'DOC3'), 0.00429490875745564),\n",
       " (('of', 'DOC3'), 0.03865417881710076),\n",
       " (('passages', 'DOC3'), 0.00429490875745564),\n",
       " (('Ipsum', 'DOC3'), 0.0214745437872782),\n",
       " (('available,', 'DOC3'), 0.00429490875745564),\n",
       " (('but', 'DOC3'), 0.00429490875745564),\n",
       " (('the', 'DOC3'), 0.030064361302189483),\n",
       " (('majority', 'DOC3'), 0.00429490875745564),\n",
       " (('have', 'DOC3'), 0.00429490875745564),\n",
       " (('suffered', 'DOC3'), 0.00429490875745564),\n",
       " (('alteration', 'DOC3'), 0.00429490875745564),\n",
       " (('some', 'DOC3'), 0.00429490875745564),\n",
       " (('form,', 'DOC3'), 0.00429490875745564),\n",
       " (('by', 'DOC3'), 0.00858981751491128),\n",
       " (('injected', 'DOC3'), 0.00858981751491128),\n",
       " (('humour,', 'DOC3'), 0.00858981751491128),\n",
       " (('or', 'DOC3'), 0.00858981751491128),\n",
       " (('randomised', 'DOC3'), 0.00429490875745564),\n",
       " (('words', 'DOC3'), 0.00858981751491128),\n",
       " (('which', 'DOC3'), 0.00858981751491128),\n",
       " ((\"don't\", 'DOC3'), 0.00429490875745564),\n",
       " (('look', 'DOC3'), 0.00429490875745564),\n",
       " (('even', 'DOC3'), 0.00429490875745564),\n",
       " (('slightly', 'DOC3'), 0.00429490875745564),\n",
       " (('believable.', 'DOC3'), 0.00429490875745564),\n",
       " (('If', 'DOC3'), 0.00429490875745564),\n",
       " (('you', 'DOC3'), 0.00858981751491128),\n",
       " (('going', 'DOC3'), 0.00429490875745564),\n",
       " (('to', 'DOC3'), 0.01717963502982256),\n",
       " (('use', 'DOC3'), 0.00429490875745564),\n",
       " (('a', 'DOC3'), 0.030064361302189483),\n",
       " (('passage', 'DOC3'), 0.00429490875745564),\n",
       " (('Ipsum,', 'DOC3'), 0.00429490875745564),\n",
       " (('need', 'DOC3'), 0.00429490875745564),\n",
       " (('be', 'DOC3'), 0.00858981751491128),\n",
       " (('sure', 'DOC3'), 0.00429490875745564),\n",
       " (('there', 'DOC3'), 0.00429490875745564),\n",
       " ((\"isn't\", 'DOC3'), 0.00429490875745564),\n",
       " (('anything', 'DOC3'), 0.00429490875745564),\n",
       " (('embarrassing', 'DOC3'), 0.00429490875745564),\n",
       " (('hidden', 'DOC3'), 0.00429490875745564),\n",
       " (('middle', 'DOC3'), 0.00429490875745564),\n",
       " (('text.', 'DOC3'), 0.00429490875745564),\n",
       " (('All', 'DOC3'), 0.00429490875745564),\n",
       " (('generators', 'DOC3'), 0.00429490875745564),\n",
       " (('on', 'DOC3'), 0.00858981751491128),\n",
       " (('Internet', 'DOC3'), 0.00429490875745564),\n",
       " (('tend', 'DOC3'), 0.00429490875745564),\n",
       " (('repeat', 'DOC3'), 0.00429490875745564),\n",
       " (('predefined', 'DOC3'), 0.00429490875745564),\n",
       " (('chunks', 'DOC3'), 0.00429490875745564),\n",
       " (('as', 'DOC3'), 0.00429490875745564),\n",
       " (('necessary,', 'DOC3'), 0.00429490875745564),\n",
       " (('making', 'DOC3'), 0.00429490875745564),\n",
       " (('this', 'DOC3'), 0.00429490875745564),\n",
       " (('first', 'DOC3'), 0.00429490875745564),\n",
       " (('true', 'DOC3'), 0.00429490875745564),\n",
       " (('generator', 'DOC3'), 0.00429490875745564),\n",
       " (('Internet.', 'DOC3'), 0.00429490875745564),\n",
       " (('It', 'DOC3'), 0.00429490875745564),\n",
       " (('uses', 'DOC3'), 0.00429490875745564),\n",
       " (('dictionary', 'DOC3'), 0.00429490875745564),\n",
       " (('over', 'DOC3'), 0.00429490875745564),\n",
       " (('200', 'DOC3'), 0.00429490875745564),\n",
       " (('Latin', 'DOC3'), 0.00429490875745564),\n",
       " (('words,', 'DOC3'), 0.00429490875745564),\n",
       " (('combined', 'DOC3'), 0.00429490875745564),\n",
       " (('with', 'DOC3'), 0.00429490875745564),\n",
       " (('handful', 'DOC3'), 0.00429490875745564),\n",
       " (('model', 'DOC3'), 0.00429490875745564),\n",
       " (('sentence', 'DOC3'), 0.00429490875745564),\n",
       " (('structures,', 'DOC3'), 0.00429490875745564),\n",
       " (('generate', 'DOC3'), 0.00429490875745564),\n",
       " (('looks', 'DOC3'), 0.00429490875745564),\n",
       " (('reasonable.', 'DOC3'), 0.00429490875745564),\n",
       " (('The', 'DOC3'), 0.00858981751491128),\n",
       " (('generated', 'DOC3'), 0.00429490875745564),\n",
       " (('is', 'DOC3'), 0.01288472627236692),\n",
       " (('therefore', 'DOC3'), 0.00429490875745564),\n",
       " (('always', 'DOC3'), 0.00429490875745564),\n",
       " (('free', 'DOC3'), 0.00429490875745564),\n",
       " (('from', 'DOC3'), 0.00429490875745564),\n",
       " (('repetition,', 'DOC3'), 0.00429490875745564),\n",
       " (('non-characteristic', 'DOC3'), 0.00429490875745564),\n",
       " (('etc.', 'DOC3'), 0.00858981751491128),\n",
       " (('long', 'DOC3'), 0.011637103773650303),\n",
       " (('established', 'DOC3'), 0.011637103773650303),\n",
       " (('fact', 'DOC3'), 0.011637103773650303),\n",
       " (('that', 'DOC3'), 0.023274207547300606),\n",
       " (('reader', 'DOC3'), 0.011637103773650303),\n",
       " (('will', 'DOC3'), 0.011637103773650303),\n",
       " (('distracted', 'DOC3'), 0.011637103773650303),\n",
       " (('readable', 'DOC3'), 0.011637103773650303),\n",
       " (('content', 'DOC3'), 0.011637103773650303),\n",
       " (('page', 'DOC3'), 0.011637103773650303),\n",
       " (('when', 'DOC3'), 0.011637103773650303),\n",
       " (('looking', 'DOC3'), 0.011637103773650303),\n",
       " (('its', 'DOC3'), 0.011637103773650303),\n",
       " (('layout.', 'DOC3'), 0.011637103773650303),\n",
       " (('point', 'DOC3'), 0.011637103773650303),\n",
       " (('using', 'DOC3'), 0.011637103773650303),\n",
       " (('it', 'DOC3'), 0.011637103773650303),\n",
       " (('has', 'DOC3'), 0.011637103773650303),\n",
       " (('more-or-less', 'DOC3'), 0.011637103773650303),\n",
       " (('normal', 'DOC3'), 0.011637103773650303),\n",
       " (('distribution', 'DOC3'), 0.023274207547300606),\n",
       " (('letters', 'DOC3'), 0.011637103773650303)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "@ctx_inject\n",
    "def clear_ctx(ctx):\n",
    "    ctx.docname = None\n",
    "    ctx.doc_tn = {}\n",
    "    ctx.sn = {}\n",
    "    ctx.tf = {}\n",
    "\n",
    "\n",
    "clear_ctx()\n",
    "\n",
    "\n",
    "@ctx_inject\n",
    "def MAP_1(ctx, docname: str, contentns: str):\n",
    "    ctx.sn[docname] = 0\n",
    "\n",
    "    def prepare_word(word: str):\n",
    "        new_word = word.replace('\\n', '')\n",
    "        for symb in r\"\"\"!\"#$%&()*+,-./:;=?@[]^_{}~\"\"\".split():\n",
    "            new_word.replace(symb, '')\n",
    "        return new_word\n",
    "    for word in contentns.split(' '):\n",
    "        ctx.sn[docname] += 1\n",
    "        yield ((prepare_word(word,), docname), 1)\n",
    "\n",
    "\n",
    "@ctx_inject\n",
    "def REDUCE_1(ctx, par):\n",
    "    for p in par:\n",
    "        wd, cnt = p\n",
    "        word, doc = wd\n",
    "        if ctx.docname is None:\n",
    "            ctx.docname = doc\n",
    "        if doc != ctx.docname:\n",
    "            for word, tn in ctx.doc_tn.items():\n",
    "                tf = tn / ctx.sn[ctx.docname]\n",
    "                yield ((word, ctx.docname), tf)\n",
    "            ctx.docname = doc\n",
    "        dtnc = ctx.doc_tn.get(word, 0) + cnt\n",
    "        ctx.doc_tn[word] = dtnc\n",
    "\n",
    "    for word, tn in ctx.doc_tn.items():\n",
    "        tf = tn / ctx.sn[ctx.docname]\n",
    "        yield ((word, ctx.docname), tf)\n",
    "\n",
    "\n",
    "job_1 = map(lambda x: x, (map(REDUCE_1, map(MAP_1, docs, contents))))\n",
    "\n",
    "\n",
    "def MAP_2(inp):\n",
    "    ret = {}\n",
    "    for obj in inp:\n",
    "        wd, tf = obj\n",
    "        word, doc = wd\n",
    "        ret[word] = (doc, tf, 1)\n",
    "    for word, meta in ret.items():\n",
    "        yield (word, (meta))\n",
    "\n",
    "\n",
    "@ctx_inject\n",
    "def REDUCE_2(ctx, inp):\n",
    "    docname = None\n",
    "    for d in inp:\n",
    "        word, dti = d\n",
    "        doc, tf, cnt = dti\n",
    "        docname = doc\n",
    "        _, cur_n = ctx.tf.get((word), [tf, 0])\n",
    "        cur_n += cnt\n",
    "        ctx.tf[(word)] = [tf, cur_n]\n",
    "    for word, tf_cnt in ctx.tf.items():\n",
    "        yield (word, docname), (*tf_cnt,)\n",
    "\n",
    "\n",
    "job_2 = map(REDUCE_2, map(MAP_2, job_1))\n",
    "\n",
    "\n",
    "def MAP_3(inp):\n",
    "    for word_doc, tf_n in inp:\n",
    "        word, doc = word_doc\n",
    "        tf, n = tf_n\n",
    "        yield ((word, doc), tf * math.log10(len(docs) / n))\n",
    "\n",
    "\n",
    "job_3 = list(map(MAP_3, list(job_2)))\n",
    "list(flatten(list(job_3)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ssau3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
